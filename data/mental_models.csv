id;page_content
1; "Search Medium Write 99+ Sven Balnojan Marcadamwillis Member-only story Mental Models For The Data Space Build your data company faster, manage your data products better, and invest smarter by using data-specific mental models Sven Balnojan Geek Culture Sven Balnojan Published in Geek Culture · 8 min read · Mar 26 9 1 To a man with a hammer, everything looks like a nail. Let me give you a toolkit instead! “In Munger’s view, it is better to be worldly wise than to spend lots of time working with a single model that is precisely wrong. A multiple-model approach that is only approximately right will produce a far better outcome in anything that involves people or a social system.” (Tren Griffin in “Charlie Munger: The Complete Investor”) Legendary investor & thinker Charlie Munger, the man with the “quickest 30-second mind” according to Warren Buffet, is direct and clear in his approach. He uses a multi-model approach to decision-making. A large collection of models coming from different disciplines to quickly make good proximate decisions. I’ve spent the last couple of years collecting a lot of mental models relevant to the data space. I’ve used them as a data product manager to make product decisions. I’m using them every day in my newsletter “Three Data Point Thursday” to understand and evaluate data companies, and data trends. I’m using them in my job at Meltano to understand users every day. I always use them to quickly get to the bottom and to make a good decision without needing much more feedback or weeks of research. And you should too. How do use a mental model? Mental models are not a replacement for customer research or investment due diligence. They don’t replace experimenting with new trends. They offer you a shortcut, and help you move in the right direction, but never give you a clear path. Just an idea. You then use customer research, due diligence, experimentation, and feedback to find the right path, knowing that you are going in the right direction. Or you get feedback that contradicts the model you used and realize your assumptions were wrong. But since you have not just one, but a whole catalog of models, you’re quickly able to course-correct and choose a different model with fitting assumptions. Mental models help you to move fast by making decisions fast. If you face a decision: Try a model, check its assumptions, think about the outcome, then possibly choose a different model with better fitting assumptions to your current situation. That’s how you use mental models. That’s why you need many, contexts to change, you need to exchange your current model of thinking all the time. These 11 mental models are the ones I constantly use in the data space. Backward induction Backward induction is a tool from game theory. It works backward. Essentially you first determine the optimal state, win the game and then work backward through the decisions that led to this state. There are many more game theoretical reasoning tactics. This one works best in games where the space of options is huge on every turn. In these situations, you don’t see the tree for the forest. Data isn’t a hammer, in itself, it is not useful. Only resulting actions are. Thus investors, data product managers, and startup founders tend to get lost in the forest. Backward induction for data always starts with people taking better/faster actions thanks to data. First-principle thinking The first principles are the building blocks of knowledge. Reasoning or thinking from first principles means taking apart and abstracting away all the context, getting to a few core ideas. Then you solve these using the first principles. Software is a young discipline, data is way younger. So in contrast to the general body of knowledge, the lifetime and the knowledge that exists in the data space is tiny. When people realized we need to up our productivity inside data teams, they took the problem apart and landed on DataOps. DataOps describes itself as being inspired by ideas from much older disciplines like software, DevOps, agile, and manufacturing. This is the first principle of thinking applied. Unlearning Unlearning means assuming current knowledge is wrong. Whether this is a good assumption or not, if you assume current knowledge is wrong, you’re able to then think about different alternatives, and open up your mind. Data is growing exponentially, and the data space is moving fast. That in turn means, most knowledge and most practices are outdated. Most ideas do not apply to most situations. It means the word “best practices” really doesn’t apply to anything in the data space. We might have “good practices” that might work given certain situations, but there simply are no “best practices”. Do you think adopting dbt is a “best practice”? Try to assume it is not, and think about what else you could do. Then apply backward induction to start with your goal (delivering better decisions to your company) instead of a fancy tech trend. Actions work without data This model is simple: People will take action, even if you do not supply them with data at all — it’s called a random walk. They take action with little data, with crappy data. So if you’re building a fancy new data-based product, there is always the alternative to not use it at all, and use whatever data people have at their hands. Heck, they might use a mental model and simply take a guess. It’s not like PayPal where when you don’t have a “delete account” option, people are not able to delete an account. If people do not have your recommendation engine, they will still decide to buy something. So this model is connected to the working backward model. Data PMs building fancy new recommendation engines often make the mistake of assuming “this will drive up sales by helping people to choose”. But people likely are already choosing, using top-sold lists, human written recommendations on the internet. Their competition is not “nothing” it is there and very real. Datacisions Cycle The datacisions cycle is the “infinite” cycle of data. People (or machines, or animals,…) take action, and these generate data if someone collects it, the data is then turned into information, and the information produces insights into a human's mind (or machine) this insight turns into a decision that finally turns into another data-producing action. You’re using this cycle to understand the value of products you’re building, and of companies, you want to invest in. Take for instance a tracking solution. I love tracking solutions, but they only work on one tiny piece of the cycle. As such, they depend on everything else and need to be integrated into the complete cycle to truly deliver value. Thus I prefer a slimmer tracking solution that also already integrates itself into more parts of the cycle over a fat one that does just tracking. Bottlenecks Bottlenecks are the places where if we push more through, the output is still the same. They “inhibit flow”. Bottlenecks are so relevant to the data industry because data is digital. And for digital things, you can track flow. Bottlenecks are also easy to track inside the datacisions cycle. These two models are connected there. Data analysts often become bottlenecks inside companies. No matter how much data you push onto them, they aren’t able to turn more than they can into insights for their decision-makers. If that’s a problem you see in your company, you don’t need to do anything at all about more data, you need to help your data analysts. Pie Making Pie-making is a concept known I’ve learned in the web3 space. If we have a limited number of players that are about to grab a pie, there are two ways of getting the most. One is to steal them from others. The second it to make the pie bigger! That idea is relevant to e-commerce, where Amazon since day one was focused on growing the pie rather than taking the (back then) super small piece. It is relevant to the data space because the pie still is tiny. The company Dbtlabs started the Analytics Engineering movement thereby turning less data-aware people into more data-aware people and thus is growing the pie. Walk me through it Einstein famously solved complex problems with thought experiments. Because it is quite hard to sit on top of a light beam. And even the experiments to confirm the relativity theory took years to finish. Luckily, we’re in a better place in the data world. Yes, it is always helpful to take the user's perspective and really go through the steps necessary to take an action. It will quickly help you to drill deeper into the model “Actions work without data”. But in the data world, you often have an even better tool at hand: The question “show me how you do it today.”. It’s that simple. I’m always amazed at how many people build data-related products for people without first seeing firsthand how they do whatever they are solving for today. Because “Actions work without data” should tell you, they do solve their problem somehow. Systems Thinking Systems thinking contrasts problem thinking. This mental model says it is better to solve a systemic problem, than an individual one. It’s better to help people build data applications than launch the next notebook startup. It means you value companies higher that solve systemic problems. It means, if you’re evaluating the next cool data tool, you should think about evaluating the next cool data trend & culture shift instead. Data Vision Data visions are related to first principle thinking. If everything we know is mostly wrong, then a good product cannot be based on what we know, and what everyone knows. Then a good data-related product has to be embedded into a larger vision for the whole space. I’m not saying you have to have that vision, you might as well ride on Dbtlabs “Analytics Engineering is the way of the future” kind of story. But you do need to embed your product or company into one. Reduce complexity Complexity & complexity in general is vast concepts. What I like to use is the following very specific idea: You can reduce overall system complexity by making individual parts smaller and less dependent. When to use this model: This model only works, if you gain from less dependence when you gain from “smaller parts”. It works for microservices if you have separate teams watching after these “parts”. Then fewer dependencies are great. This plays out as data-oriented programming where people separate data and code from each other, making each part individually easier to understand. There are many more mental models I love to use, manufacturing ideas, software delivery pipelines, value streams, and the cynefin framework. All of Munger's frameworks work amazingly well inside the data space. But these are the 11 that I tend to use most often and find to have the broadest applicability. That might change in the future, but for now, I suggest you give some of them a try and build up your own latticework of mental models. Interested in how to build great data companies, great data-heavy products, becoming a great data team, or how to build anything great with open source? Then consider joining my free newsletter “Three Data Point Thursday”. It’s become a trusted resource for data start-ups, VCs, and data leaders. Interested in data engineering in general? I share my favorite 6 articles of the week every week in my free newsletter “Finish Slime”. Data Big Data Data Science AI Product Manager 9 1 Sven Balnojan Geek Culture Written by Sven Balnojan 1.91K Followers · Writer for Geek Culture DataOps @ Meltano | Data PM | “Data Mesh in Action” | Join my free data newsletters at http://thdpth.com/ and http://finishslime.com Edit profile More from Sven Balnojan and Geek Culture Data Mesh — How to Make Data Governance “Computational” Sven Balnojan Sven Balnojan in Towards Data Science Data Mesh — How to Make Data Governance “Computational” In his article, I’ll explain a simple framework that will help you working on the “computational” parts of your data governance inside a… · 7 min read · Nov 17, 2022 62 4 Binary Neural Networks: A Game Changer in Machine Learning Vikas Kumar Ojha Vikas Kumar Ojha in Geek Culture Binary Neural Networks: A Game Changer in Machine Learning This blog explains about binary neural networks which have potential of revolutionizing deep learning if proper efforts are made. · 9 min read · Feb 19 239 1 Understanding the 10 Most Difficult Python Concepts Joanna Joanna in Geek Culture Understanding the 10 Most Difficult Python Concepts Python has a simple syntax and is easy to learn, making it an ideal choice for beginners. However, as you delve deeper into Python, you may… · 8 min read · Apr 3 196 Building a Data Mesh on Databricks — Fast Sven Balnojan Sven Balnojan in Towards Data Science Building a Data Mesh on Databricks — Fast How to build a data mesh platform inside databricks. · 6 min read · Nov 7, 2022 133 4 See all from Sven Balnojan See all from Geek Culture Recommended from Medium Strategies to Turn Data into Insights Patrick Nguyen Patrick Nguyen Strategies to Turn Data into Insights Getting valuable and useful information from data · 3 min read · May 25 151 Is ChatGPT getting dumber? Let’s talk about ‘AI Drift’ Jim the AI Whisperer Jim the AI Whisperer in The Generator Is ChatGPT getting dumber? Let’s talk about ‘AI Drift’ Why is AI losing its edge? AI Drift explained 🚗💨🤖 · 8 min read · Aug 16 1.7K 26 Lists Predictive Modeling w/ Python 20 stories · 296 saves Databricks role-based and specialty certification line-up. New_Reading_List 174 stories · 77 saves Image by vectorjuice on FreePik The New Chatbots: ChatGPT, Bard, and Beyond 13 stories · 89 saves What is ChatGPT? 9 stories · 156 saves users gathered around computer screens showing an ecommerce site. Jake Holmquist Jake Holmquist in Google Cloud - Community Exploring the new Data Quality Dashboard in Google Discovery AI And how to use Generative AI in BigQuery to Generate Product Catalog Descriptions and Improve Data Quality 12 min read · Aug 13 18 1 HTML REPORT Prathamesh Gadekar Prathamesh Gadekar in Level Up Coding Python Libraries for Lazy Data Scientists Do you feel lethargic today? Use these five libraries to boost your productivity. 7 min read · Apr 7 645 3 Data Modelling — a reflection Azlan Nazari Azlan Nazari Data Modelling — a reflection My journey in understanding data modelling as a BI developer. 7 min read · Aug 6 Data Vault on Snowflake: Expanding to Dimensional Models Patrick Cuba Patrick Cuba in Dev Genius Data Vault on Snowflake: Expanding to Dimensional Models Snowflake continues to set the standard for Data in the Cloud by taking away the need to perform maintenance tasks on your data platform… · 11 min read · 6 days ago 21 See more recommendations Help Status Writers Blog Careers Privacy Terms About Text to speech Teams"